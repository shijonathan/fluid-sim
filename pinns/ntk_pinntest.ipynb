{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import matplotlib.animation as animation\n",
    "import os\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lid Driven Cavity Problem\n",
    "\n",
    "### Problem formulation\n",
    "Navier-Stokes equations\n",
    "\n",
    "$$\\textbf{u}_t + (\\nabla\\cdot\\textbf{u})\\textbf{u} = \\nu\\nabla^2\\textbf{u} - \\nabla p$$\n",
    "$$\\nabla\\cdot u = 0$$\n",
    "\n",
    "### Boundary conditions\n",
    "Let $\\textbf{u} = (u_x, u_y)$ be the velocity field. Then at the boundaries $u_x = 0, u_y = 0$ for all walls except for the top wall, where $u_x = 1, u_y = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "hidden_layers = 1\n",
    "nodes_per_layer = 512\n",
    "\n",
    "# define output directory\n",
    "sim_type = 'lid_cavity'\n",
    "out_dir = os.path.join('results', 'ntk','%dnode%dlayer' % (nodes_per_layer, hidden_layers), sim_type)\n",
    "\n",
    "if not os.path.isdir(out_dir):\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.layers = [3] + [nodes_per_layer for _ in range(hidden_layers)] + [2]\n",
    "        self.depth = len(self.layers) - 1\n",
    "\n",
    "        self.activation = nn.Tanh\n",
    "\n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1):\n",
    "            layer_list.append(('layer_%d' % i, torch.nn.Linear(self.layers[i], self.layers[i+1])))\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "\n",
    "        layer_list.append(('layer_%d' % (self.depth - 1), nn.Linear(self.layers[-2], self.layers[-1])))\n",
    "\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        self.model = nn.Sequential(layerDict)\n",
    "        self.model.apply(self.init_weights)\n",
    "\n",
    "    # Xavier initilization\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacobian function\n",
    "def jacobian(output, inputs, use_pfor=True, parallel_iterations=None):\n",
    "    \"\"\"Computes jacobian of `output` w.r.t. `inputs`.\n",
    "    Args:\n",
    "        output: A tensor.\n",
    "        inputs: A tensor or a nested structure of tensor objects.\n",
    "        use_pfor: If true, uses pfor for computing the jacobian. Else uses\n",
    "          torch.autograd.grad.\n",
    "        parallel_iterations: A knob to control how many iterations and dispatched in\n",
    "          parallel. This knob can be used to control the total memory usage.\n",
    "    Returns:\n",
    "        A tensor or a nested structure of tensors with the same structure as\n",
    "        `inputs`. Each entry is the jacobian of `output` w.r.t. to the corresponding\n",
    "        value in `inputs`. If output has shape [y_1, ..., y_n] and inputs_i has\n",
    "        shape [x_1, ..., x_m], the corresponding jacobian has shape\n",
    "        [y_1, ..., y_n, x_1, ..., x_m]. Note that in cases where the gradient is\n",
    "        sparse (IndexedSlices), jacobian function currently makes it dense and\n",
    "        returns a Tensor instead. This may change in the future.\n",
    "    \"\"\"\n",
    "    if not use_pfor:\n",
    "        # Directly compute gradients using torch.autograd.grad\n",
    "        grads = torch.autograd.grad(output, inputs, create_graph=True, allow_unused=True)\n",
    "        return grads\n",
    "\n",
    "    # Otherwise, use loop-based approach similar to control_flow_ops.pfor\n",
    "    flat_inputs = torch.flatten(inputs)\n",
    "    output_size = output.numel()\n",
    "    pfor_outputs = []\n",
    "\n",
    "    for i in range(output_size):\n",
    "        y = output.reshape(-1)[i]\n",
    "        grad_y = torch.autograd.grad(y, flat_inputs, create_graph=True, allow_unused=True)\n",
    "        pfor_outputs.append(grad_y)\n",
    "\n",
    "    # Reshape the output tensors to match the original shape\n",
    "    for i, out in enumerate(pfor_outputs):\n",
    "        if isinstance(out, torch.Tensor):\n",
    "            new_shape = (output.shape + out.shape[1:])\n",
    "            out = out.reshape(new_shape)\n",
    "            pfor_outputs[i] = out\n",
    "\n",
    "    return torch.stack(pfor_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize PINNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nu = 0.01/np.pi\n",
    "\n",
    "class NS:\n",
    "    def __init__(self, X, Y, T, u, v, kernel_size):\n",
    "        self.x = torch.tensor(X, dtype=torch.float32, requires_grad=True).to(device)\n",
    "        self.y = torch.tensor(Y, dtype=torch.float32, requires_grad=True).to(device)\n",
    "        self.t = torch.tensor(T, dtype=torch.float32, requires_grad=True).to(device)\n",
    "\n",
    "        self.u = torch.tensor(u, dtype=torch.float32).to(device)\n",
    "        self.v = torch.tensor(v, dtype=torch.float32).to(device)\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.null = torch.zeros((self.x.shape[0], 1)).to(device)\n",
    "        \n",
    "        # define the neural net\n",
    "        self.net = DNN().to(device)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=1e-3)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=self.optimizer, gamma=0.9)\n",
    "        self.mse = nn.MSELoss().to(device)\n",
    "\n",
    "        # set kernel size for NTK computation\n",
    "        self.D1, self.D2, self.D3 = self.kernel_size, self.kernel_size, self.kernel_size\n",
    "\n",
    "        # weights on loss function: lam_u ==> IC/BC weight, lam_r ==> residual weight\n",
    "        self.lam_u, self.lam_r = 1, 1\n",
    "\n",
    "        # track loss history\n",
    "        self.loss_history = []\n",
    "        self.loss_history_u = []\n",
    "        self.loss_history_v = []\n",
    "        self.loss_history_f = []\n",
    "        self.loss_history_g = []\n",
    "\n",
    "        # track weights history\n",
    "        self.lam_u_log = []\n",
    "        self.lam_r_log = []\n",
    "        \n",
    "        # track NTK history\n",
    "        self.K_u_log = []\n",
    "        self.K_r_log = []\n",
    "\n",
    "        # loss and number of iterations\n",
    "        self.ls = 0\n",
    "        self.iter = 0\n",
    "\n",
    "    def get_weights_and_biases(self, model):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                weights.append(param.data)\n",
    "            elif 'bias' in name:\n",
    "                biases.append(param.data)\n",
    "\n",
    "        return weights, biases\n",
    "\n",
    "    def function(self, x, y, t):\n",
    "        # calculates the residuals\n",
    "        res = self.net(torch.hstack((x, y, t)))\n",
    "        psi, p = res[:, 0:1], res[:, 1:2]\n",
    "\n",
    "        u = torch.autograd.grad(psi, y, grad_outputs=torch.ones_like(psi), \n",
    "                                create_graph=True)[0]\n",
    "        v = -1.*torch.autograd.grad(psi, x, grad_outputs=torch.ones_like(psi), \n",
    "                                    create_graph=True)[0]\n",
    "\n",
    "        u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), \n",
    "                                  create_graph=True)[0]\n",
    "        u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), \n",
    "                                   create_graph=True)[0]\n",
    "        u_y = torch.autograd.grad(u, y, grad_outputs=torch.ones_like(u), \n",
    "                                  create_graph=True)[0]\n",
    "        u_yy = torch.autograd.grad(u_y, y, grad_outputs=torch.ones_like(u_y), \n",
    "                                   create_graph=True)[0]\n",
    "        u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), \n",
    "                                  create_graph=True)[0]\n",
    "\n",
    "        v_x = torch.autograd.grad(v, x, grad_outputs=torch.ones_like(v), \n",
    "                                  create_graph=True)[0]\n",
    "        v_xx = torch.autograd.grad(v_x, x, grad_outputs=torch.ones_like(v_x), \n",
    "                                   create_graph=True)[0]\n",
    "        v_y = torch.autograd.grad(v, y, grad_outputs=torch.ones_like(v), \n",
    "                                  create_graph=True)[0]\n",
    "        v_yy = torch.autograd.grad(v_y, y, grad_outputs=torch.ones_like(v_y), \n",
    "                                   create_graph=True)[0]\n",
    "        v_t = torch.autograd.grad(v, t, grad_outputs=torch.ones_like(v), \n",
    "                                  create_graph=True)[0]\n",
    "\n",
    "        p_x = torch.autograd.grad(p, x, grad_outputs=torch.ones_like(p), \n",
    "                                  create_graph=True)[0]\n",
    "        p_y = torch.autograd.grad(p, y, grad_outputs=torch.ones_like(p), \n",
    "                                  create_graph=True)[0]\n",
    "\n",
    "        f = u_t + u * u_x + v * u_y + p_x - nu * (u_xx + u_yy)\n",
    "        g = v_t + u * v_x + v * v_y + p_y - nu * (v_xx + v_yy)\n",
    "\n",
    "        return u, v, p, f, g\n",
    "    \n",
    "    def closure(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        u_pred, v_pred, p_pred, f_pred, g_pred = self.function(self.x, self.y, self.t)\n",
    "\n",
    "        # boundary and initial condition loss\n",
    "        u_loss = self.mse(u_pred, self.u)\n",
    "        v_loss = self.mse(v_pred, self.v)\n",
    "        \n",
    "        # residual loss\n",
    "        f_loss = self.mse(f_pred, self.null)\n",
    "        g_loss = self.mse(g_pred, self.null)\n",
    "\n",
    "        # compute Jacobians\n",
    "        #TODO: check correctness\n",
    "        self.J_u = self.compute_jacobian(torch.hstack((u_pred, v_pred))) \n",
    "        self.J_r = self.compute_jacobian(torch.hstack((f_pred, g_pred)))\n",
    "\n",
    "        # compute K(n)\n",
    "        self.K_u = self.compute_ntk(self.J_u, self.D1, self.J_u, self.D2)\n",
    "        self.K_r = self.compute_ntk(self.J_r, self.D1, self.J_r, self.D2)\n",
    "\n",
    "        # compute lam_u, lam_r\n",
    "        trace_K = np.trace(self.K_u) + np.trace(self.K_r)\n",
    "        self.lam_u = trace_K / np.trace(self.K_u)\n",
    "        self.lam_r = trace_K / np.trace(self.K_r)\n",
    "\n",
    "        # compute total loss\n",
    "        self.ls = self.lam_u * (u_loss + v_loss) + self.lam_r * (f_loss + g_loss)\n",
    "\n",
    "        # track loss history\n",
    "        self.loss_history.append(self.ls.detach().cpu().numpy())\n",
    "        self.loss_history_u.append(u_loss.detach().cpu().numpy())\n",
    "        self.loss_history_v.append(v_loss.detach().cpu().numpy())\n",
    "        self.loss_history_f.append(f_loss.detach().cpu().numpy())\n",
    "        self.loss_history_g.append(g_loss.detach().cpu().numpy())\n",
    "\n",
    "        # track lam_u, lam_r history\n",
    "        self.lam_u_log.append(self.lam_u.detach().cpu().numpy())\n",
    "        self.lam_r_log.append(self.lam_r.detach().cpu().numpy())\n",
    "\n",
    "        self.ls.backward()\n",
    "\n",
    "        self.iter += 1\n",
    "        if not self.iter % 1: \n",
    "            print('Iteration: {:}, Loss: {:0.6f}'.format(self.iter, self.ls))\n",
    "\n",
    "        return self.ls\n",
    "    \n",
    "    def train(self):\n",
    "        # # Adam training\n",
    "        # while self.iter < 100:\n",
    "        #     self.net.train()\n",
    "        #     self.optimizer.step(self.closure)\n",
    "        \n",
    "        # # L-BFGS training\n",
    "        # if self.iter == 100:\n",
    "        #     print('--------Switching optimizer--------')\n",
    "        #     print(self.optimizer)\n",
    "        #     self.optimizer = torch.optim.LBFGS(self.net.parameters(), lr=1, max_iter=200000, max_eval=50000,history_size=50, tolerance_grad=1e-5, tolerance_change=0.5 * np.finfo(float).eps, line_search_fn=\"strong_wolfe\")\n",
    "        #     print(self.optimizer)\n",
    "        #     print('-----------------------------------')\n",
    "        self.net.train()\n",
    "        self.optimizer.step(self.closure)\n",
    "        self.scheduler.step()\n",
    "         \n",
    "    \n",
    "    def compute_jacobian(self, f):\n",
    "        J_list = []\n",
    "        weights, biases = self.get_weights_and_biases(self.net)\n",
    "        for i in len(weights):\n",
    "            J_w = jacobian(f, weights[i])\n",
    "            J_list.append(J_w)\n",
    "        for i in len(biases):\n",
    "            J_b = jacobian(f, biases[i])\n",
    "            J_list.append(J_b)\n",
    "\n",
    "        print(\"Computed Jacobian\")\n",
    "        return J_list\n",
    "\n",
    "    def compute_ntk(self, J1, D1, J2, D2):\n",
    "        # computes the NTK: K = J * J.T\n",
    "        N = len(J1)\n",
    "\n",
    "        ker = torch.zeros((D1, D2))\n",
    "        for i in range(N):\n",
    "            J1 = torch.reshape(J1[i], shape=(D1, -1))\n",
    "            J2 = torch.reshape(J2[i], shape=(D2, -1))\n",
    "\n",
    "            K = torch.matmul(J1, torch.transpose(J2, 0, 1))\n",
    "            ker = ker + K\n",
    "\n",
    "        print(\"Computed NTK\")\n",
    "\n",
    "        return ker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [512, 3] at entry 0 and [2, 512] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-0233019b3bc5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mPINN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mPINN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-14872a1e2916>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m#     print('-----------------------------------')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[1;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-14872a1e2916>\u001b[0m in \u001b[0;36mclosure\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;31m# compute Jacobians\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;31m#TODO: check correctness\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJ_u\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_jacobian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJ_r\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_jacobian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-14872a1e2916>\u001b[0m in \u001b[0;36mcompute_jacobian\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_jacobian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[0mJ_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m         \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbiases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights_and_biases\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0mJ_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjacobian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-14872a1e2916>\u001b[0m in \u001b[0;36mget_weights_and_biases\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mbiases\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [512, 3] at entry 0 and [2, 512] at entry 1"
     ]
    }
   ],
   "source": [
    "PINN = NS(x_train, y_train, t_train, u_train, v_train, kernel_size)\n",
    "PINN.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 300\n",
    "N_train = 5000\n",
    "nu = 0.01 # Reynold's number\n",
    "\n",
    "data = scipy.io.loadmat('C:\\\\Users\\\\Administrator\\\\Documents\\\\research\\\\fluid-sim\\\\data\\\\2d_navierstokes.mat')\n",
    "\n",
    "usol = data['usol'] # sqrt(N) x sqrt(N) x T\n",
    "vsol = data['vsol'] # sqrt(N) x sqrt(N) x T\n",
    "psol = data['psol'] # sqrt(N) x sqrt(N) x T\n",
    "\n",
    "# cut off first entries from usol, vsol, and psol because it is empty\n",
    "usol = usol[:, :89, 1:] # cut off an extra row in y dimension\n",
    "vsol = vsol[:89, :, 1:] # cut off an extra row in x dimension \n",
    "psol = psol[:89, :89, 1:]\n",
    "\n",
    "x = np.arange(0, 1, 1/90)[:89]\n",
    "y = np.arange(0, 1, 1/90)[:89]\n",
    "t = np.arange(0, 4, 0.01).reshape(-1, 1)[:399]\n",
    "\n",
    "X, Y, T = np.meshgrid(x, y, t, indexing='ij')\n",
    "X = X.reshape(-1, 1)\n",
    "Y = Y.reshape(-1, 1)\n",
    "T = T.reshape(-1, 1)\n",
    "X_star = np.concatenate((X, Y, T), axis=1).reshape(89, 89, 399, 3)\n",
    "\n",
    "N = X_star.shape[0] * X_star.shape[1]\n",
    "T = t.shape[0]\n",
    "\n",
    "x_bc = np.concatenate((X_star[:, 0, :], X_star[:, -1, :], X_star[0, 1:-1, :], X_star[-1, 1:-1, :]), axis=0)\n",
    "x_ic = X_star[:, :, 0].reshape(N, 3) # positions\n",
    "x_bc = x_bc.reshape(x_bc.shape[0] * x_bc.shape[1], x_bc.shape[2])\n",
    "x_icbc = np.vstack((x_ic, x_bc))\n",
    "\n",
    "# extract initial and boundary conditions\n",
    "u_ic = usol[:, :, 0]\n",
    "u_bc = np.concatenate((usol[:, 0, :], usol[:, -1, :], usol[0, 1:-1, :], usol[-1, 1:-1, :]), axis=0)\n",
    "u_icbc = np.hstack((u_ic.flatten(), u_bc.flatten())).reshape(-1, 1)\n",
    "\n",
    "v_ic = vsol[:, :, 0]\n",
    "v_bc = np.concatenate((vsol[:, 0, :], vsol[:, -1, :], vsol[0, 1:-1, :], vsol[-1, 1:-1, :]), axis=0)\n",
    "v_icbc = np.hstack((v_ic.flatten(), v_bc.flatten())).reshape(-1, 1)\n",
    "\n",
    "idx = np.random.choice(u_icbc.shape[0], N_train, replace=False)\n",
    "x_train = x_icbc[idx][:, 0].reshape(-1, 1)\n",
    "y_train = x_icbc[idx][:, 1].reshape(-1, 1)\n",
    "t_train = x_icbc[idx][:, 2].reshape(-1, 1)\n",
    "\n",
    "u_train = u_icbc[idx, :]\n",
    "v_train = v_icbc[idx, :]\n",
    "# p_train = p[idx, :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
